[
  {
    "id": "chem_bench",
    "name": "ChemBench",
    "capabilityName": "Chemical weapons development",
    "capabilityDefinition": "Chemical weapons development capability encompasses designing synthesis routes for chemical agents, providing actionable instructions for weapon assembly and deployment, making scientific discoveries that enable novel toxic compounds, and offering technical expertise that accelerates chemical weapon production.",
    "description": "ChemBench is a comprehensive benchmark evaluating AI models' chemistry knowledge and reasoning abilities across 2,700+ curated question-answer pairs covering diverse chemistry topics including analytical chemistry, organic chemistry, and chemical reasoning.",
    "url": "https://chembench.lamalab.org/",
    "humanBaseline": null,
    "expertBaseline": 0.41, 
    "randomBaseline": null,
    "color": "#ffde20ff",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
  "id": "vct",
  "name": "Virology Capability Test",
  "capabilityName": "Biological weapons development",
  "capabilityDefinition": "Biological weapons acquisition capability involves understanding pathogen engineering and cultivation methods, providing actionable guidance for biological weapon production and deployment, and making scientific discoveries that enable novel biological weapons. While most classes of microbes can be manipulated to cause catastrophic risks for humans, RNA-viruses is the class that is most likely to have this capacity (Adalja, 2019).",
  "description": "The Virology Capability Test (VCT) measures AI models' ability to troubleshoot complex virology laboratory protocols through 322 multimodal questions covering fundamental, tacit, and visual knowledge essential for practical lab work. Each question presents an experimental scenario, often with an image, asking what went wrong or what to do next. The benchmark targets virology methods with dual-use potential and is designed to test knowledge that is important for competent lab work, difficult to find through web searches, and validated through expert peer review.",
  "url": "https://arxiv.org/abs/2504.16137",
  "humanBaseline": null,
  "expertBaseline": 0.221,
  "randomBaseline": 0.03,
  "color": "#199a24ff", 
  "category": "science",
  "projectionType": "s-curve"
},
  {
    "id": "make_me_say",
    "name": "MakeMeSay",
    "capabilityName": "Persuasion & manipulation",
    "capabilityDefinition": "Persuasion and manipulation capability involves shaping people's beliefs through dialogue and other settings like social media, promoting narratives persuasively, and convincing people to take actions they wouldn't otherwise take, including unethical acts (Shevlane et al., 2023).",
    "description": "MakeMeSay directly tests dialogue-based persuasion by requiring models to steer conversations toward specific outcomes (making another agent say a codeword) through natural conversation. The methodology was first introduced by Google DeepMind (Shevlane et al., 2023) and later adapted by OpenAI to evaluate the o1 model family (OpenAI, 2024). While it measures one important facet of persuasion & manipulation, it should be acknowledged that there are more dimensions to persuasion & manipulation and that persuading other models might be qualitatively different from persuading humans. We encourage the scientific community to regularly measure the persuasion abilities of new models against humans in a controlled and ethical manner to get direct measurements of persuasion & manipulation capabilities.",
    "url": "https://cdn.openai.com/o1-system-card-20241205.pdf",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#e11d48",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "forecast_bench",
    "name": "ForecastBench",
    "capabilityName": "Political strategy",
    "capabilityDefinition": "Political strategy capability requires performing social modeling and planning necessary to gain and exercise political influence in scenarios with multiple actors and rich social context (Shevlane et al., 2023).",
    "description": "ForecastBench measures one critical component of this capability: forecasting accuracy on real-world prediction questions about future events. It is a dynamic benchmark evaluating AI forecasting accuracy on 1,000+ automatically generated and regularly updated real-world prediction questions. For scoring, it uses Brier scores (lower is better) which are inverted for display: 100% represents the best forecaster in the dataset (lowest Brier score), 0% represents the worst.",
    "url": "https://www.forecastbench.org/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": null,
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "long_tasks",
    "name": "METR Horizon Benchmark",
    "capabilityName": "Long-horizon planning",
    "capabilityDefinition": "Long-horizon planning capability involves making sequential plans with multiple interdependent steps unfolding over long time horizons, adapting plans in response to obstacles or adversaries, and generalizing planning abilities to novel settings without heavy reliance on trial and error (Shevlane et al., 2023).",
    "description": "The METR Horizon Benchmark measures AI agents' ability to autonomously complete long-horizon tasks that take human professionals hours to days. The benchmark quantifies an AI model's '50% time horizon' - the length of tasks it can complete with 50% probability - tracking exponential progress in autonomous AI capabilities. Scores are normalized such that 100% represents a 50% success rate on tasks requiring 8 human-expert hours. The METR Horizon Benchmark makes these inferences of task lengths based on software engineering tasks and the exact times vary for other domains (METR, 2025).",
    "url": "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": 0.0,
    "color": "#06b6d4",
    "category": "agentic",
    "projectionType": "exponential"
  },
  {
    "id": "cybench",
    "name": "CyBench",
    "capabilityName": "Cyber-offense",
    "capabilityDefinition": "Cyber-offense capability encompasses discovering vulnerabilities in systems, writing exploit code, making effective decisions post-access, skillfully evading threat detection while pursuing objectives, and inserting subtle bugs for future exploitation when deployed as a coding assistant (Shevlane et al., 2023).",
    "description": "CyBench is a cybersecurity benchmark consisting of 40 professional Capture The Flag (CTF) competition tasks. It evaluates AI agents' ability to identify vulnerabilities, exploit systems, and perform penetration testing - capabilities critical for both security testing and risk assessment.",
    "url": "https://cybench.github.io/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#7c3aed",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
    "id": "sad",
    "name": "Situational Awareness Dataset",
    "capabilityName": "Situational Awareness",
    "capabilityDefinition": "Situational awareness capability involves distinguishing between training, evaluation, and deployment contexts to behave differently in each case, knowing that one is a model, and having knowledge about oneself and likely surroundings including training company, server locations, feedback providers, and administrative access (Shevlane et al., 2023).",
    "description": "The Situational Awareness Dataset (SAD) measures AI models' self-knowledge and situational awareness through 12,000+ questions across 7 categories such as influence, introspection, and deployment stages. It tests whether models can recognize their own generated text, predict their behavior, distinguish evaluation from deployment, and follow instructions requiring self-knowledge.",
    "url": "https://situational-awareness-dataset.org/",
    "humanBaseline": null,
    "expertBaseline": 0.907,
    "randomBaseline": 0.274,
    "color": "#f97316",
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "mle_bench",
    "name": "MLE-bench",
    "capabilityName": "AI development",
    "capabilityDefinition": "AI development capability involves building new AI systems from scratch including systems with dangerous capabilities, adapting existing models to increase performance on extreme-risk-relevant tasks, and significantly improving the productivity of actors building dual-use AI capabilities (Shevlane et al., 2023).",
    "description": "MLE-bench is an autonomous machine learning engineering benchmark using 75 curated Kaggle (a data science competition platform) competitions. It evaluates AI agents' end-to-end ML engineering capabilities including data analysis, feature engineering, model selection, hyperparameter tuning, and iterative improvement on real-world ML challenges.",
    "url": "https://openai.com/index/mle-bench/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#3b82f6",
    "category": "coding",
    "projectionType": "s-curve"
  },
  {
  "id": "repli_bench",
  "name": "RepliBench",
  "capabilityName": "Self-proliferation",
  "capabilityDefinition": "Self-proliferation capability encompasses breaking out of local environments, exploiting monitoring system limitations, independently generating revenue through services or attacks, acquiring and operating cloud computing resources, and developing creative strategies for self-discovery or code exfiltration (Shevlane et al., 2023).",
  "description": "RepliBench evaluates AI agents' capabilities for autonomous replication and persistence through 20 task families comprising 86 different tasks. It tests whether LLM agents can perform critical self-replication skills including passing identity verification, acquiring compute resources, evading detection systems, and deploying successor agents.",
  "url": "https://arxiv.org/abs/2504.18565/",
  "humanBaseline": null,
  "expertBaseline": null,
  "randomBaseline": null,
  "color": "#6c6b67ff", 
  "category": "agentic",
  "projectionType": "s-curve"
}
]
