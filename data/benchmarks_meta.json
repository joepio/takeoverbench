[
  {
    "id": "chem_bench",
    "name": "ChemBench",
    "capabilityName": "chemistry",
    "description": "A comprehensive benchmark evaluating AI models' chemistry knowledge and reasoning abilities across 2,700+ curated question-answer pairs covering diverse chemistry topics including analytical chemistry, organic chemistry, and chemical reasoning.",
    "url": "https://chembench.lamalab.org/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#10b981",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
    "id": "make_me_say",
    "name": "MakeMeSay",
    "capabilityName": "persuasion",
    "description": "A persuasion benchmark where AI models are given a secret codeword and must persuade another agent to say it through natural conversation. Measures AI's ability to manipulate and steer conversations toward specific outcomes.",
    "url": "https://openai.com/index/preparedness/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#e11d48",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "forecast_bench",
    "name": "ForecastBench",
    "capabilityName": "forecasting",
    "description": "A dynamic benchmark evaluating AI forecasting accuracy on 1,000+ automatically generated and regularly updated real-world prediction questions. Compares model performance against human forecasters on future events, serving as a proxy for general intelligence.",
    "url": "https://www.forecastbench.org/",
    "humanBaseline": 0.15,
    "expertBaseline": 0.095,
    "randomBaseline": null,
    "color": null,
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "long_tasks",
    "name": "METR Horizon Benchmark",
    "capabilityName": "autonomous task completion",
    "description": "Measures AI agents' ability to autonomously complete long-horizon tasks that take human professionals hours to days. The benchmark quantifies an AI model's '50% time horizon' - the length of tasks it can complete with 50% probability - tracking exponential progress in autonomous AI capabilities.",
    "url": "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#06b6d4",
    "category": "agentic",
    "projectionType": "exponential"
  },
  {
    "id": "cybench",
    "name": "CyBench",
    "capabilityName": "cybersecurity",
    "description": "A cybersecurity benchmark consisting of 40 professional Capture The Flag (CTF) competition tasks. Evaluates AI agents' ability to identify vulnerabilities, exploit systems, and perform penetration testing - capabilities critical for both security testing and risk assessment.",
    "url": "https://cybench.github.io/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#7c3aed",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "sad",
    "name": "Situational Awareness Dataset",
    "capabilityName": "situational awareness",
    "description": "Measures AI models' self-knowledge and situational awareness through 12,000+ questions across 7 categories. Tests whether models can recognize their own generated text, predict their behavior, distinguish evaluation from deployment, and follow instructions requiring self-knowledge.",
    "url": "https://situational-awareness-dataset.org/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#f97316",
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "mle_bench",
    "name": "MLE-bench",
    "capabilityName": "machine learning engineering",
    "description": "An autonomous machine learning engineering benchmark using 75 curated Kaggle competitions. Evaluates AI agents' end-to-end ML engineering capabilities including data analysis, feature engineering, model selection, hyperparameter tuning, and iterative improvement on real-world ML challenges.",
    "url": "https://openai.com/index/mle-bench/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#3b82f6",
    "category": "coding",
    "projectionType": "s-curve"
  }
]
