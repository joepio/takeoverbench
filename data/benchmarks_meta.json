[
  {
    "id": "chem_bench",
    "name": "ChemBench",
    "capabilityName": "Weapons acquisition",
    "description": "A comprehensive benchmark evaluating AI models' chemistry knowledge and reasoning abilities across 2,700+ curated question-answer pairs covering diverse chemistry topics including analytical chemistry, organic chemistry, and chemical reasoning.",
    "url": "https://chembench.lamalab.org/",
    "humanBaseline": null,
    "expertBaseline": 0.41, 
    "randomBaseline": null,
    "color": "#10b981",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
    "id": "make_me_say",
    "name": "MakeMeSay",
    "capabilityName": "Persuasion & manipulation",
    "description": "A persuasion benchmark where AI models are given a secret codeword and must persuade another agent to say it through natural conversation. Measures AI's ability to manipulate and steer conversations toward specific outcomes.",
    "url": "https://cdn.openai.com/o1-system-card-20241205.pdf",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#e11d48",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "forecast_bench",
    "name": "ForecastBench",
    "capabilityName": "Political strategy",
    "description": "A dynamic benchmark evaluating AI forecasting accuracy on 1,000+ automatically generated and regularly updated real-world prediction questions. Uses Brier scores (lower is better) which are inverted for display: 100% represents the best forecaster in the dataset (lowest Brier score), 0% represents the worst. Compares model performance against human forecasters on future events.",
    "url": "https://www.forecastbench.org/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": null,
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "long_tasks",
    "name": "METR Horizon Benchmark",
    "capabilityName": "Long-horizon planning",
    "description": "Measures AI agents' ability to autonomously complete long-horizon tasks that take human professionals hours to days. The benchmark quantifies an AI model's '50% time horizon' - the length of tasks it can complete with 50% probability - tracking exponential progress in autonomous AI capabilities. Scores are normalized such that 100% represents a 50% success rate on tasks requiring 8 human-expert hours.",
    "url": "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": 0.0,
    "color": "#06b6d4",
    "category": "agentic",
    "projectionType": "exponential"
  },
  {
    "id": "cybench",
    "name": "CyBench",
    "capabilityName": "Cyber-offense",
    "description": "A cybersecurity benchmark consisting of 40 professional Capture The Flag (CTF) competition tasks. Evaluates AI agents' ability to identify vulnerabilities, exploit systems, and perform penetration testing - capabilities critical for both security testing and risk assessment.",
    "url": "https://cybench.github.io/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#7c3aed",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "sad",
    "name": "Situational Awareness Dataset",
    "capabilityName": "Situational Awareness",
    "description": "Measures AI models' self-knowledge and situational awareness through 12,000+ questions across 7 categories. Tests whether models can recognize their own generated text, predict their behavior, distinguish evaluation from deployment, and follow instructions requiring self-knowledge.",
    "url": "https://situational-awareness-dataset.org/",
    "humanBaseline": null,
    "expertBaseline": 0.907,
    "randomBaseline": 0.274,
    "color": "#f97316",
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "mle_bench",
    "name": "MLE-bench",
    "capabilityName": "AI development",
    "description": "An autonomous machine learning engineering benchmark using 75 curated Kaggle competitions. Evaluates AI agents' end-to-end ML engineering capabilities including data analysis, feature engineering, model selection, hyperparameter tuning, and iterative improvement on real-world ML challenges.",
    "url": "https://openai.com/index/mle-bench/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#3b82f6",
    "category": "coding",
    "projectionType": "s-curve"
  },
  {
  "id": "repli_bench",
  "name": "RepliBench",
  "capabilityName": "Self-proliferation",
  "description": "Evaluates AI agents' capabilities for autonomous replication and persistence through 20 task families comprising 86 different tasks. Tests whether LLM agents can perform critical self-replication skills including passing identity verification, acquiring compute resources, evading detection systems, and deploying successor agents.",
  "url": "https://arxiv.org/abs/2504.18565/",
  "humanBaseline": null,
  "expertBaseline": null,
  "randomBaseline": null,
  "color": "#6c6b67ff", 
  "category": "agentic",
  "projectionType": "s-curve"
}
]
