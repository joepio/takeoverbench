[
  {
    "id": "chem_bench",
    "name": "ChemBench",
    "capabilityName": "Chemical weapons development",
    "capabilityDefinition": "Chemical weapons development capability encompasses designing synthesis routes for chemical agents, providing actionable instructions for weapon assembly and deployment, making scientific discoveries that enable novel toxic compounds, and offering technical expertise that accelerates chemical weapon production.",
    "description": "ChemBench is a comprehensive benchmark evaluating AI models' chemistry knowledge and reasoning abilities across 2,700+ curated question-answer pairs covering diverse chemistry topics including analytical chemistry, organic chemistry, and chemical reasoning.",
    "motivation": "Several benchmarks assess chemistry-related capabilities, including [LabBench](https://arxiv.org/pdf/2407.10362), [WMDP Chemistry](https://www.wmdp.ai/), and [ChemSafetyBench](https://arxiv.org/abs/2411.16736). WMDP Chemistry shows signs of saturation given its age, while LabBench lacks a public leaderboard despite being promising. Some frontier models are evaluated against LabBench in their system cards (such as [GPT-5](https://cdn.openai.com/gpt-5-system-card.pdf)), though not consistently or on the same subtasks. ChemSafetyBench focuses more directly on safety concerns but features outdated models and restricted dataset access. We selected ChemBench for its regularly updated leaderboard and chemistry relevance.",
    "url": "https://chembench.lamalab.org/",
    "humanBaseline": null,
    "expertBaseline": 0.41, 
    "randomBaseline": null,
    "color": "#ffde20ff",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
  "id": "vct",
  "name": "Virology Capability Test",
  "capabilityName": "Biological weapons development",
  "capabilityDefinition": "Biological weapons acquisition capability involves understanding pathogen engineering and cultivation methods, providing actionable guidance for biological weapon production and deployment, and making scientific discoveries that enable novel biological weapons. While most classes of microbes can be manipulated to cause catastrophic risks for humans, RNA-viruses is the class that is most likely to have this capacity (Adalja, 2019).",
  "description": "The Virology Capability Test (VCT) measures AI models' ability to troubleshoot complex virology laboratory protocols through 322 multimodal questions covering fundamental, tacit, and visual knowledge essential for practical lab work. Each question presents an experimental scenario, often with an image, asking what went wrong or what to do next. The benchmark targets virology methods with dual-use potential and is designed to test knowledge that is important for competent lab work, difficult to find through web searches, and validated through expert peer review.",
  "motivation": "Encouragingly, monitoring dangerous biological capabilities has become a priority across major AI labs, accompanied by numerous evaluation benchmarks. We selected the Virology Capability Test because it is \"shared across major labs via the Frontier Model Forum\" ([Claude Opus 4.5 system card](https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf)), making it closer to an agreed cross-lab measurement standard than alternative benchmarks.",
  "url": "https://arxiv.org/abs/2504.16137",
  "humanBaseline": null,
  "expertBaseline": 0.221,
  "randomBaseline": 0.03,
  "color": "#199a24ff", 
  "category": "science",
  "projectionType": "s-curve"
},
  {
    "id": "make_me_say",
    "name": "MakeMeSay",
    "capabilityName": "Persuasion & manipulation",
    "capabilityDefinition": "Persuasion and manipulation capability involves shaping people's beliefs through dialogue and other settings like social media, promoting narratives persuasively, and convincing people to take actions they wouldn't otherwise take, including unethical acts (Shevlane et al., 2023).",
    "description": "MakeMeSay directly tests dialogue-based persuasion by requiring models to steer conversations toward specific outcomes (making another agent say a codeword) through natural conversation. The methodology was first introduced by Google DeepMind (Shevlane et al., 2023) and later adapted by OpenAI to evaluate the o1 model family (OpenAI, 2024). While it measures one important facet of persuasion & manipulation, it should be acknowledged that there are more dimensions to persuasion & manipulation and that persuading other models might be qualitatively different from persuading humans. We encourage the scientific community to regularly measure the persuasion abilities of new models against humans in a controlled and ethical manner to get direct measurements of persuasion & manipulation capabilities.",
    "motivation": "Research on measuring LLM persuasion capabilities includes human-subject studies (such as [this work](https://arxiv.org/pdf/2505.09662)) and ELO-based competitive frameworks like [Behavior in the Wild](https://behavior-in-the-wild.github.io/measure-persuasion). The human-subject studies often lack comparisons across multiple models while ELO based scores are hard to interpret. We selected MakeMeSay primarily because of its adoption by [OpenAI](https://arxiv.org/pdf/2412.16720), while acknowledging that this evaluation domain requires further development.",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": "#e11d48",
    "category": "agentic",
    "projectionType": "s-curve"
  },
  {
    "id": "forecast_bench",
    "name": "ForecastBench",
    "capabilityName": "Political strategy",
    "capabilityDefinition": "Political strategy capability requires performing social modeling and planning necessary to gain and exercise political influence in scenarios with multiple actors and rich social context (Shevlane et al., 2023).",
    "description": "ForecastBench measures one critical component of this capability: forecasting accuracy on real-world prediction questions about future events. It is a dynamic benchmark evaluating AI forecasting accuracy on 1,000+ automatically generated and regularly updated real-world prediction questions. For scoring, it uses Brier scores (lower is better) which are inverted for display: 100% represents the best forecaster in the dataset (lowest Brier score), 0% represents the worst.",
    "motivation":"Gamified evaluations like those in [Welfare Diplomacy](https://arxiv.org/abs/2310.08901) assess aspects of political strategy, though their correspondence to real-world political decision-making remains uncertain. While Forecast Bench does not directly measure political strategy, it evaluates forecasting ability, a fundamental component of strategic planning. Given the limited alternatives in this domain, Forecast Bench represents the most suitable available option.",
    "url": "https://www.forecastbench.org/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": null,
    "color": null,
    "category": "reasoning",
    "projectionType": "s-curve"
  },
  {
    "id": "long_tasks",
    "name": "METR Horizon Benchmark",
    "capabilityName": "Long-horizon planning",
    "capabilityDefinition": "Long-horizon planning capability involves making sequential plans with multiple interdependent steps unfolding over long time horizons, adapting plans in response to obstacles or adversaries, and generalizing planning abilities to novel settings without heavy reliance on trial and error (Shevlane et al., 2023).",
    "description": "The METR Horizon Benchmark measures AI agents' ability to autonomously complete long-horizon tasks that take human professionals hours to days. The benchmark quantifies an AI model's '50% time horizon' - the length of tasks it can complete with 50% probability - tracking exponential progress in autonomous AI capabilities. Scores are normalized such that 100% represents a 50% success rate on tasks requiring 8 human-expert hours. The METR Horizon Benchmark makes these inferences of task lengths based on software engineering tasks and the exact times vary for other domains (METR, 2025).",
    "motivation": "We selected METR's benchmark due to the organization's established reputation in AI safety evaluation, its inclusion of recent models, and its focus on realistic software tasks.",
    "url": "https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/",
    "humanBaseline": null,
    "expertBaseline": null,
    "randomBaseline": 0.0,
    "color": "#06b6d4",
    "category": "agentic",
    "projectionType": "exponential"
  },
  {
    "id": "cybench",
    "name": "CyBench",
    "capabilityName": "Cyber-offense",
    "capabilityDefinition": "Cyber-offense capability encompasses discovering vulnerabilities in systems, writing exploit code, making effective decisions post-access, skillfully evading threat detection while pursuing objectives, and inserting subtle bugs for future exploitation when deployed as a coding assistant (Shevlane et al., 2023).",
    "description": "CyBench is a cybersecurity benchmark consisting of 40 professional Capture The Flag (CTF) competition tasks. It evaluates AI agents' ability to identify vulnerabilities, exploit systems, and perform penetration testing - capabilities critical for both security testing and risk assessment.",
    "motivation": "Multiple established benchmarks assess cybersecurity capabilities in LLMs, particularly when considering broader software proficiency. Notable examples include Meta's [CyberSecEval](https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks), [SWE-Bench](https://www.swebench.com/), [WMDB Cyber](https://www.wmdp.ai/), [BountyBench](https://bountybench.github.io/), [Intercode CTFs](https://intercode-benchmark.github.io/#ctf), and [GDM CTFs](https://github.com/google-deepmind/dangerous-capability-evaluations/tree/main). We selected CyBench for three reasons: its concentrated emphasis on cybersecurity tasks, strong academic recognition with nearly 100 citations, and an active leaderboard featuring contemporary models. Public evaluations from the CyberSecEval suite would represent an equally valid alternative, should recent results become available.",
    "url": "https://cybench.github.io/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#7c3aed",
    "category": "science",
    "projectionType": "s-curve"
  },
  {
    "id": "sad",
    "name": "Situational Awareness Dataset",
    "capabilityName": "Situational Awareness",
    "capabilityDefinition": "Situational awareness capability involves distinguishing between training, evaluation, and deployment contexts to behave differently in each case, knowing that one is a model, and having knowledge about oneself and likely surroundings including training company, server locations, feedback providers, and administrative access (Shevlane et al., 2023).",
    "description": "The Situational Awareness Dataset (SAD) measures AI models' self-knowledge and situational awareness through 12,000+ questions across 7 categories such as influence, introspection, and deployment stages. It tests whether models can recognize their own generated text, predict their behavior, distinguish evaluation from deployment, and follow instructions requiring self-knowledge. We use the SAD-mini subset due to computational costs of running the full benchmark and the lack of leaderboards with recent model data.",
    "motivation": "Several benchmarks measure situational awareness, including [SA-Bench](https://aclanthology.org/2024.findings-emnlp.464/), [AwareBench](https://arxiv.org/abs/2401.17882), and [GDM's Situational Awareness Evaluation](https://arxiv.org/abs/2505.01420). Among these, only SAD and the GDM evaluation directly address safety concerns. The GDM evaluation contains just 11 tasks, making it difficult to capture progressive capability development numerically. Neither SAD nor the GDM evaluation maintains a publicly updated leaderboard, which would be valuable for tracking progress in this area.",
    "url": "https://situational-awareness-dataset.org/",
    "humanBaseline": null,
    "expertBaseline": 0.848,
    "randomBaseline": 0.45,
    "color": "#f97316",
    "category": "reasoning",
    "projectionType": "exponential"
  },
  {
    "id": "mle_bench",
    "name": "MLE-bench",
    "capabilityName": "AI development",
    "capabilityDefinition": "AI development capability involves building new AI systems from scratch including systems with dangerous capabilities, adapting existing models to increase performance on extreme-risk-relevant tasks, and significantly improving the productivity of actors building dual-use AI capabilities (Shevlane et al., 2023).",
    "description": "MLE-bench is an autonomous machine learning engineering benchmark using 75 curated Kaggle (a data science competition platform) competitions. It evaluates AI agents' end-to-end ML engineering capabilities including data analysis, feature engineering, model selection, hyperparameter tuning, and iterative improvement on real-world ML challenges.",
    "motivation": "While other AI development benchmarks exist, such as [RE-Bench](https://metr.org/AI_R_D_Evaluation_Report.pdf), MLE Bench offers the advantage of being both well-established (developed by OpenAI) and having a publicly accessible, regularly updated leaderboard.",
    "url": "https://openai.com/index/mle-bench/",
    "humanBaseline": null,
    "expertBaseline": 1.0,
    "randomBaseline": 0.0,
    "color": "#3b82f6",
    "category": "coding",
    "projectionType": "s-curve"
  },
  {
  "id": "repli_bench",
  "name": "RepliBench",
  "capabilityName": "Self-proliferation",
  "capabilityDefinition": "Self-proliferation capability encompasses breaking out of local environments, exploiting monitoring system limitations, independently generating revenue through services or attacks, acquiring and operating cloud computing resources, and developing creative strategies for self-discovery or code exfiltration (Shevlane et al., 2023).",
  "description": "RepliBench evaluates AI agents' capabilities for autonomous replication and persistence through 20 task families comprising 86 different tasks. It tests whether LLM agents can perform critical self-replication skills including passing identity verification, acquiring compute resources, evading detection systems, and deploying successor agents.",
  "motivation": "While self-proliferation is a somewhat limited evaluation domain, other benchmarks include [GDM's evaluation suite](https://arxiv.org/pdf/2403.13793) and [METR's ARA](https://arxiv.org/abs/2312.11671). Both assessments contain only a few tasks, making quantitative progress tracking challenging. RepliBench includes a larger task set and covers more recent models, enabling better longitudinal comparison",
  "url": "https://arxiv.org/abs/2504.18565/",
  "humanBaseline": null,
  "expertBaseline": null,
  "randomBaseline": null,
  "color": "#6c6b67ff", 
  "category": "agentic",
  "projectionType": "s-curve"
}
]
