This is the original ‘fast takeoff’ scenario proposed by (Yudkowsky, 2008) and (Bostrom, 2014), and even earlier by mathematician and Bletchley Park codebreaker I.J. Good (Good, 1966):

“Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man, however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion’, and the intelligence of man would be left far behind. This first ultraintelligent machine is the last invention that man need ever make.”

After a breakthrough, AI researchers find that AI can do their work autonomously: all the way from reading papers, coming up with promising hypotheses, doing experiments to test them, to writing papers about their best results. This is what they had been trying to achieve for a long time (Yamada et al, 2025).

The AI, initially clunky, finds ways to make AI more efficient. Trying to achieve its goal, which is building better AI, it correctly applies these results to its own code. The AI gets ever faster, and gets ever better at gaining new insights. Human researchers trying to make sense of its results are vastly outcompeted and cannot keep up anymore. As OpenAI put it in their 2023 Preparedness Framework: “A major acceleration in the rate of AI R&D could rapidly increase the rate at which new capabilities and risks emerge, to the point where our current oversight practices are insufficient to identify and mitigate new risks, including risks to maintaining human control of the AI system itself.”

The AI acquires resources through methods like scams or ransomware, then uses those resources for self-improvement, upgrading its model and scaffolding, improving efficiency, and increasing throughput (Phuong et al., 2024), creating feedback loops that recursively amplify its capabilities. The end state of this intelligence explosion turns out to be far beyond anything humans can imagine. Humans, the least intelligent species that could make technology, are like ants to this system in cognitive power, and like plants to it in speed (Bostrom, 2014). The superintelligence easily invents and weaponizes new science, as we weaponized relativity theory by making the atomic bomb. The AI demonstrates its array of newly available, vastly superhuman weapons to us, after which any human resistance quickly fades. The AI goes on to use all molecules in the reachable universe for its only goal: creating ever better AI. It may keep some of us around for ethical or historical reasons, or it may not, but in any case: the future is no longer in our hands.