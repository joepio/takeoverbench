This is the original ‘fast takeoff’ scenario proposed by Eliezer Yudkowsky and Nick Bostrom, and even earlier by mathematician and Bletchley Park codebreaker I.J. Good in the 1960s:

“Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man, however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion’, and the intelligence of man would be left far behind. This first ultraintelligent machine is the last invention that man need ever make.”

After a breakthrough, AI researchers find that AI can do AI development autonomously: all the way from reading papers, coming up with promising hypotheses, doing experiments to test them, to writing papers about their best results. This is what they had been trying to achieve for a long time.

The AI, initially clunky, finds ways to make AI more efficient. Trying to achieve its goal, which is building better AI, it correctly applies these results to its own code. The AI gets ever faster, and gets ever better at gaining new insights. Human researchers trying to make sense of its results are vastly outcompeted and cannot keep up anymore.

The end state of this intelligence explosion turns out to be far beyond anything humans can imagine. Humans, the least intelligent species that could make technology, are like ants to this system in cognitive power, and like plants to it in speed. The superintelligence easily invents and weaponizes new science, as we weaponized relativity theory by making the atomic bomb. The AI demonstrates its array of newly available, vastly superhuman weapons to us, after which any human resistance quickly fades. The AI goes on to use all molecules in the reachable universe for its only goal: creating ever better AI. It may keep some of us around for ethical or historical reasons, or it may not, but in any case: the future is no longer in our hands.